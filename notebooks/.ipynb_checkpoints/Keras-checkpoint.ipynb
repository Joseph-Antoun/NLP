{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-6338f42df26c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecurrent\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGRU\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense\n",
    "from keras.layers import GlobalAveragePooling1D, Input, Conv1D, MaxPooling1D, Flatten\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>29995</td>\n",
       "      <td>I have no idea what's going on this trailer an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>29996</td>\n",
       "      <td>I misread that at David Cross, and now I'm try...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>29997</td>\n",
       "      <td>Well lets be reasonable next time and dont unb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>29998</td>\n",
       "      <td>Jaime dumping on Jon for going off to serve in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>29999</td>\n",
       "      <td>I think he'll be on par, but more mechanic tha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                           comments\n",
       "29995  29995  I have no idea what's going on this trailer an...\n",
       "29996  29996  I misread that at David Cross, and now I'm try...\n",
       "29997  29997  Well lets be reasonable next time and dont unb...\n",
       "29998  29998  Jaime dumping on Jon for going off to serve in...\n",
       "29999  29999  I think he'll be on par, but more mechanic tha..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = '../data/reddit_train.csv'\n",
    "file_path2 = '../data/reddit_test.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "data = data.drop(columns={'id'})\n",
    "data.tail()\n",
    "test_data = pd.read_csv(file_path2)\n",
    "test_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(s):\n",
    "    for expr in [r\"</d>\",r\"</s>\",r\"[^A-Za-z0-9(),!?\\'\\`]\"]:\n",
    "        s = re.sub(expr, \" \", s)\n",
    "    for expr in [r\"\\'s\",r\"\\'ve\",r\"\\'t\",r\"\\'re\",r\"\\'d\",r\"\\'11\",]:\n",
    "        s = re.sub(expr, \" \"+expr[1:], s)\n",
    "    for expr in [r\",\",r\"!\",r\"\\(\",r\"\\)\"r\"\\?\"]:\n",
    "        s = re.sub(expr, \" \"+expr[1:]+\" \", s)\n",
    "    s = re.sub(r\"\\s{2,}\", \" \", s)\n",
    "    s = re.sub(r'\\S*(x{2,}|X{2,})\\S*', \"xxx\", s)\n",
    "    s = re.sub(r'[^\\x00-\\x7F]+', \"\", s)\n",
    "    return s.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"comments\"] = data[\"comments\"].apply(lambda x: clean_data(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tqdm_notebook' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-268c5a513d22>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m  \u001b[1;34m'conspiracy'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'worldnews'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wow'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;36m17\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                            'europe' : 18, 'canada' : 19, 'Music' : 20, 'baseball' : 21})\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tqdm_notebook' is not defined"
     ]
    }
   ],
   "source": [
    "for c in data[\"subreddits\"].unique():\n",
    "    data[c] = np.zeros(len(data)).astype(int)\n",
    "u = data[\"subreddits\"].map({'hockey': 2, 'nba' : 3, 'leagueoflegends' : 4, 'soccer' : 5, 'funny': 6,\n",
    "                           'movies' : 7, 'anime' : 8,\n",
    " 'Overwatch': 9, 'trees' : 10, 'GlobalOffensive' : 11, 'nfl' : 12, 'AskReddit' : 13, 'gameofthrones' : 14,\n",
    " 'conspiracy' : 15, 'worldnews' : 16, 'wow' : 17,\n",
    "                           'europe' : 18, 'canada' : 19, 'Music' : 20, 'baseball' : 21})\n",
    "for i in tqdm_notebook(range(len(data))):\n",
    "    data.iloc[i,u[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data.comments, data[data['subreddits'].unique()],\n",
    "                                                   test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xLengths = [len(word_tokenize(x)) for x in X_train]\n",
    "h = sorted(xLengths)\n",
    "maxLength = h[len(h) - 1]\n",
    "print(\"max input lenght is: \", maxLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxLength = h[int(len(h) * 0.8)]\n",
    "print(\"80% covers input sequence length up to \", maxLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_size = 200000\n",
    "input_tokenizer = Tokenizer(max_vocab_size)\n",
    "input_tokenizer.fit_on_texts(X_train)\n",
    "input_vocab_size = len(input_tokenizer.word_index) +1\n",
    "print(\"input_vocab_size: \", input_vocab_size)\n",
    "totalX = np.array(pad_sequences(input_tokenizer.texts_to_sequences(X_train), maxlen=maxLength))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_categories = data['subreddits'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_vocab_size, EMBEDDING_DIM, input_length=maxLength))\n",
    "model.add(GRU(256, dropout=0.1, return_sequences=True))\n",
    "model.add(GRU(256, dropout=0.1))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(num_categories, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(totalX, y_train.values, validation_split=0.3, batch_size=128, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_size = 200000\n",
    "test_tokenizer = Tokenizer(max_vocab_size)\n",
    "test_tokenizer.fit_on_texts(X_test)\n",
    "test_vocab_size = len(test_tokenizer.word_index) + 1\n",
    "print(\"Test vocab size: \", test_vocab_size)\n",
    "totalX_test = np.array(pad_sequences(input_tokenizer.texts_to_sequences(X_test), maxlen=maxLength))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict_classes(totalX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_class = {'hockey': 2, 'nba' : 3, 'leagueoflegends' : 4, 'soccer' : 5, 'funny': 6,\n",
    "                           'movies' : 7, 'anime' : 8,\n",
    " 'Overwatch': 9, 'trees' : 10, 'GlobalOffensive' : 11, 'nfl' : 12, 'AskReddit' : 13, 'gameofthrones' : 14,\n",
    " 'conspiracy' : 15, 'worldnews' : 16, 'wow' : 17,\n",
    "                           'europe' : 18, 'canada' : 19, 'Music' : 20, 'baseball' : 21}\n",
    "inv_map = {v: k for k, v in map_class.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(pred==data.loc[y_test.index].subreddits.map(map_class).values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
